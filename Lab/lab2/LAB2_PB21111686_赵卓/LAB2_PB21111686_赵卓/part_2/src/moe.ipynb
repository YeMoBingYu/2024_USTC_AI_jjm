{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引用库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List\n",
    "import os\n",
    "import hashlib\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### 简介\n",
    "Tokenization 的主要目的是将文本分解成更小的单位(Tokens)，减小模型输入数据的内在结构复杂度(从句子变为单词序列)，从而简化模型训练的难度。同时将字符的序列转化为Token序号的序列，便于模型输入。\n",
    "\n",
    "Tokenization 首先确定语言的词表划分粒度，一般可分为：\n",
    "* 字符级：将文本分解为字符。\n",
    "* 单词级：将文本分解为单词。\n",
    "* 子词级：将单词进一步分解为更小的有意义单元（如前缀、后缀）。\n",
    "\n",
    "之后使用预定义的规则来识别 tokens, 或使用统计或机器学习技术来识别最优的 token 切分方式。例如，BPE（Byte Pair Encoding）或 SentencePiece。\n",
    "\n",
    "最后实现一组文本序列和Tokens序列之间相互转化的函数，即可完成Tokenization部分。\n",
    "\n",
    "### 实验要求\n",
    "\n",
    "1. 实现字符级切分的简单tokenizer， 由 字符表， 字符到token的 encoder()函数 和 token到字符的 decoder() 函数组成。\n",
    "2. 调用 现有的tokenizer实现，比如openai 的tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, dataPath:str):\n",
    "        with open(dataPath,\"r\",encoding=\"utf-8\") as f:\n",
    "            self.dataset = f.read()\n",
    "        self.generate_vocabulary()\n",
    "    \n",
    "    # 产生符号表\n",
    "    def generate_vocabulary(self, ):\n",
    "        self.char2index = {}\n",
    "        self.index2char = {}\n",
    "\n",
    "        # 添加开始和结束符号\n",
    "        self.char2index[\"<START>\"] = 0\n",
    "        self.char2index[\"<END>\"] = 1\n",
    "        self.index2char[0] = \"<START>\"\n",
    "        self.index2char[1] = \"<END>\"\n",
    "\n",
    "        # 对数据集中的每个字符都标号，从2开始\n",
    "        index = 2 \n",
    "        for char in sorted(set(self.dataset)):\n",
    "            if char not in self.char2index:\n",
    "                self.char2index[char] = index\n",
    "                self.index2char[index] = char\n",
    "                index += 1\n",
    "\n",
    "    # 对数据集进行编码\n",
    "    def encode(self, sentence : str) -> torch.Tensor:\n",
    "        # 添加开始符号\n",
    "        tokenflow = [self.char2index[\"<START>\"]]\n",
    "        # 对于输入句子的每个字符，依次编号添加到tokens中\n",
    "        # 如果有不在符号表中的字符，用终结符替代\n",
    "        for char in sentence:\n",
    "            tokenflow.append(self.char2index.get(char, self.char2index[\"<END>\"]))  \n",
    "        # 添加终结符号\n",
    "        tokenflow.append(self.char2index[\"<END>\"])\n",
    "        # 以Tensor形式返回  \n",
    "        return torch.tensor(tokenflow, dtype=torch.long)\n",
    "    \n",
    "    # 编码后进行解码\n",
    "    def decode(self, tokens : torch.Tensor) -> str:\n",
    "        # 转换为list\n",
    "        if isinstance(tokens, torch.Tensor):\n",
    "            tokens = tokens.tolist()\n",
    "        # 解码为字符流\n",
    "        charflow = []\n",
    "        for token in tokens:\n",
    "            # 未知编码返回<UNK>即Unknown\n",
    "            char = self.index2char.get(token, \"<UNK>\")\n",
    "            if char not in [\"<START>\", \"<END>\"]:\n",
    "                charflow.append(char)\n",
    "        # 连接成字符串返回\n",
    "        return \"\".join(charflow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义 dataloader 和 dataset\n",
    "\n",
    "为了高效加载数据，我们需要把输入文件接入 PyTorch 的数据加载器中。在这里我们定义 `ShakespeareDataset` 类用于加载数据集，用 PyTorch 的 `DataLoader` 类来实现数据加载。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, filepath, tokenizer, chunk_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        self.encoded = self.tokenizer.encode(text)\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded) - self.chunk_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 提取文本，文本作为输入，每个字符的下一个字符作为标签（size+1）\n",
    "        chunk = self.encoded[idx: idx + self.chunk_size]\n",
    "        label = self.encoded[idx + 1: idx + self.chunk_size + 1]\n",
    "        return chunk, label\n",
    "\n",
    "tokenizer = Tokenizer(dataPath=\"input.txt\")\n",
    "\n",
    "def create_dataloader(filepath, tokenizer, chunk_size, batch_size, shuffle=True):\n",
    "    dataset = ShakespeareDataset(filepath, tokenizer, chunk_size)\n",
    "    train_dataset,val_dataset = torch.utils.data.random_split(dataset,[int(len(dataset)*0.8),len(dataset)-int(len(dataset)*0.8)])\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "train_dataloader,val_dataloader = create_dataloader('input.txt', tokenizer, chunk_size=200, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意力的计算公式为：\n",
    "$$\n",
    "Head = Attention(x)=Softmax(M\\cdot QK^T)V\\\\\n",
    "Q=xW_{q},K=xW_{k}, V=xW_{v}\n",
    "$$\n",
    "这里实现的一些数学技巧可以参见attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadAttention(nn.Module):\n",
    "    def __init__(self, seq_len:int, embed_size:int, hidden_size:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(seq_len, seq_len)))\n",
    "        \n",
    "        # 初始化三个矩阵\n",
    "        self.to_q = nn.Linear(embed_size, hidden_size, bias=False)\n",
    "        self.to_k = nn.Linear(embed_size, hidden_size, bias=False)\n",
    "        self.to_v = nn.Linear(embed_size, hidden_size, bias=False)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len, embed_size = inputs.shape\n",
    "\n",
    "        # 依次计算queries,keys,values\n",
    "        queries = self.to_q(inputs) \n",
    "        keys = self.to_k(inputs)  \n",
    "        values = self.to_v(inputs)  \n",
    "\n",
    "        d_k = queries.shape[-1]\n",
    "        queries = queries / (d_k ** 0.5)\n",
    "        # 计算注意力得分\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) \n",
    "        # 实施掩码\n",
    "        mask = self.tril[:seq_len, :seq_len]\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        # 计算注意力权重\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  \n",
    "        # 得到输出\n",
    "        attention_output = torch.matmul(attention_weights, values)\n",
    "\n",
    "        return attention_output  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer中使用的注意力机制时会使用多个注意力头，期望每个注意力头能够注意到不同的信息。\n",
    "所以实际公式需要修改如下\n",
    "$$\n",
    "MultiHeadAttention(x)=[Head_0, Head_1,...,Head_h]W_o\\\\\n",
    "Head_i = Attention(x)=Softmax(M\\cdot Q_iK_i^T)V_i\\\\\n",
    "Q_i=xW_{iq},K=xW_{ik}, V=xW_{iv}\n",
    "$$\n",
    "在搭建网络的过程中，同学们可能会用到nn.ModuleList这个库，每个$Head_i$的计算可以直接使用上面已经实现的单头注意力计算。\n",
    "最后对于这些注意力头再使用一个简单的线性层/矩阵$W_o$汇总信息即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads:int, head_size:int, seq_len:int, embed_size:int):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        # 计算头的大小\n",
    "        self.head_size = embed_size // n_heads  \n",
    "        # 确保整除，分割正确\n",
    "        assert embed_size % n_heads == 0, \"attention:embed_size should divide n_heads!\"\n",
    "        # 建立多头注意力层\n",
    "        self.heads = nn.ModuleList([HeadAttention(seq_len, embed_size, self.head_size) for _ in range(n_heads)])\n",
    "        self.projection = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len, embed_size = inputs.shape\n",
    "        # 确保匹配\n",
    "        assert embed_size == self.n_heads * self.head_size, \"embed_size must be equal to n_heads * head_size\"\n",
    "        # 计算每个头的输出\n",
    "        outputs = [head(inputs) for head in self.heads] \n",
    "        # 链接所有的头输出\n",
    "        connect_output = torch.cat(outputs, dim=-1) \n",
    "        # 回到最初的大小\n",
    "        output = self.projection(connect_output)  \n",
    "        return output  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 专家网络 Expert\n",
    "\n",
    "Expert即为标准Transformer中的FeedForward模块。\n",
    "\n",
    "在经过MultiHeadAttention 模块后，seq_len中的每一个Embedding都对应了前文信息的加权求和。在经过FeedForward模块时，模型对每一个位置的Embedding进行了两次线性变换和一次非线性变换，可以视为对当前语境下的信息进行加工。知识编辑的一些研究表明，FeedForword 模块参数包含了大量的事实性知识。\n",
    "\n",
    "一个直观的想法是，类比于MultiHeadAttention，我们在每一层训练多个FeedForward模块，对于不同位置的Embedding使用不同的FeedForward模块处理对应的信息。就好像每层有多个Expert,每个Expert都负责处理一类数据的深加工，因此我们称FeedForward为Expert。\n",
    "\n",
    "实现方面:\n",
    "\n",
    "FeedForward层由两层简单的线性层组成，对于一个(batch_size, seq_len, embed_size)输入的向量x\n",
    "只在最后一个维度上进行计算，以实现词的特征维度上的交互(注意力机制是词之间的交互)。\n",
    "其首先用一个线性层将x最后一维扩大至原先4倍，然后继续用一个线性层还原回原先的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 专家层\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, embed_size: int):\n",
    "        super().__init__()\n",
    "        # 初始化两个线性层\n",
    "        self.fc1 = nn.Linear(embed_size, 4 * embed_size)\n",
    "        self.fc2 = nn.Linear(4 * embed_size, embed_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        swap = self.fc1(inputs) \n",
    "        # 实施非线性激活\n",
    "        swap = F.relu(swap) \n",
    "        outputs = self.fc2(swap) \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选通网络 TopkRouter\n",
    "\n",
    "在实现了单个Expert后，我们要设计一个选通网络决策每个Embedding要使用那个Expert计算\n",
    "\n",
    "\n",
    "### 为了说明选通网络的实现方式，我们定义一下记号：\n",
    "\n",
    "inputs.shape = [batch_size, seq_len, embed_size] = [1, 8, 16] \n",
    "\n",
    "即输入有batch_size=1个数据点，该数据有seq_len长度的context，即包含seq_len=8个Embedding，每个Embedding长度为embed_dim=16。\n",
    "\n",
    "记 num_expert = 4, 即该层包含 num_expert 个并列的Expert。\n",
    "\n",
    "记 active_expert = 2, 即计算每个Embedding仅有 active_expert 个Expert 参与计算。\n",
    "\n",
    "### 选通网络计算\n",
    "对于有seq_len=8的数据，如果每个Expert都参与计算每一个Embedding，那么一共需要计算 seq_len*embed_size = 32 次， 这极大的增加了模型计算量，因此我们往往只激活其中的active_experts个Expert，这要求我们对每一个Embedding计算最合适的active_experts个 Expert。\n",
    "\n",
    "对于单个Expert 的原版Transformer来说：\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = FeedForward(inputs[0,seq])\n",
    "$$\n",
    "\n",
    "对于多个Expert的网络：\n",
    "\n",
    "$$\n",
    "outputs[0,seq] = \\sum_{i \\in range(num\\_model)} \\alpha_{i} Expert_{i}(inputs[0,seq])\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_{i} = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "    1 & Expert_{i}  \\text{is selected} \\\\\n",
    "    0 & Expert_{i}  \\text{is not selected} \\\\\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "将$\\{\\alpha_0,\\alpha_1,\\dots,\\alpha_{num_experts-1}\\}$记为向量$\\alpha$:\n",
    "$$\n",
    "outputs[0,seq] = \\alpha \\cdot \\{Expert_i(inputs[0,seq])\\}\n",
    "$$\n",
    "\n",
    "一个选通0,2号Expert的$\\alpha$的例子是$[1,0,1,0]$\n",
    "\n",
    "问题在于如何求得 $\\alpha$, 对于一个Embedding ，我们使用神经网络对每个Expert打分，在根据分数计算$\\alpha$\n",
    "\n",
    "$$\n",
    "score[0,seq] = MLP(inputs[0,seq])  \\\\\n",
    "\\alpha = topK(score[0,seq])\n",
    "$$\n",
    "\n",
    "例如：\n",
    "\n",
    "$$\n",
    "score[0,seq] = [11.32,1.54,14.83,-1.90] \\\\\n",
    "\\alpha = [1,0,1,0]\n",
    "$$\n",
    "\n",
    "从优化的角度来说，$\\alpha$取前k大的分数的下标（即argmax），这个操作是不可导的，这里我们用之前在\"attention.ipynb\"中提到的技巧处理这里的计算。\n",
    "\n",
    "$$\n",
    "mask(score[0,seq]) = [11.32,-inf,14.83,-inf] \\\\\n",
    "\\alpha = softmax(mask(score[0,seq])) = [0.028,0,0.971,0] \\\\\n",
    "index = [1,0,1,0]\n",
    "$$\n",
    "\n",
    "我们用这个$\\alpha$和$index$用做选通网络."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, embed_size, num_experts, active_experts):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.active_experts = active_experts\n",
    "        # 对专家层进行线性变换\n",
    "        self.router_weights = nn.Linear(embed_size, num_experts)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len, embed_size = inputs.shape\n",
    "        # 为每个token计算logits\n",
    "        logits = self.router_weights(inputs)\n",
    "        # 实施softmax得到常规化结果\n",
    "        router_output = F.softmax(logits, dim=-1) \n",
    "        # 对于每个词token，选定前k个有最高权重的专家层\n",
    "        topk_values, indices = torch.topk(router_output, self.active_experts, dim=-1)\n",
    "        # 常规化选定的专家层\n",
    "        topk_normalized = topk_values / topk_values.sum(dim=-1, keepdim=True)\n",
    "        return topk_normalized, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 稀疏专家网络 SparseMoE\n",
    "\n",
    "![moe](./moeSparse.png)\n",
    "\n",
    "在定义完Expert 和 TopkRouter后，我们可以定义SparseMoE模块。\n",
    "\n",
    "在前向过程中，对于inputs.shape = [Batch_size,seq_len,embed_size]第二维度seq_len个Embedding,我们先利用TopkRouter计算出选通专家序号indices以及专家权重router_output。\n",
    "\n",
    "我们将Embedding通过选通的Expert得出active_expert个新的Embedding，然后使用router_output的作为权重对新的Embedding加权求和作为输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, embed_size: int, num_experts: int, active_experts: int):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.active_experts = active_experts\n",
    "        # 初始化router\n",
    "        self.router = TopkRouter(embed_size, num_experts, active_experts)\n",
    "        # 初始化专家层\n",
    "        self.experts = nn.ModuleList([Expert(embed_size) for _ in range(num_experts)])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size, seq_len, embed_size = inputs.shape\n",
    "        router_output, indices = self.router(inputs)\n",
    "        # 最终输出\n",
    "        final_output = torch.zeros_like(inputs)\n",
    "\n",
    "        for i in range(self.active_experts):\n",
    "            expert_idx = indices[:, :, i]\n",
    "            # 准备掩码\n",
    "            mask = torch.zeros(batch_size, seq_len, self.num_experts, device=inputs.device)\n",
    "            mask.scatter_(2, expert_idx.unsqueeze(-1), 1)\n",
    "            # 从现在的专家收集输入\n",
    "            selected_inputs = inputs.unsqueeze(2) * mask.unsqueeze(-1)\n",
    "            selected_inputs = selected_inputs.sum(dim=2) \n",
    "            # 对选定的输入实施专家层\n",
    "            expert_output = self.experts[i](selected_inputs)  \n",
    "            # 计算输出\n",
    "            final_output += expert_output * router_output[:, :, i].unsqueeze(-1) \n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer由一层层的block堆叠而成，其中每个block的结构从模型的结构图展开中可以看到，由LayerNorm，Masked multi head attention，(SparseMoE)FeedForward组成。\n",
    "\n",
    "对于一个表示句子的输入向量x，其首先会经过Layer Normalization层.\n",
    "Layer Normalization 层对于一个 句子个数x句子长度x单词向量维度 的输入 x, 会在最后两维上进行规范化处理，起到稳定训练的作用。\n",
    "\n",
    "$$\n",
    "LN(x)=\\frac{x-mean(x)}{\\sqrt{var(x)+\\epsilon}}\\cdot\\gamma+\\beta\n",
    "$$\n",
    "\n",
    "其中mean和var都是在最后两个维度上进行的，layernorm的实现同学们可以直接调用nn.LayerNorm\n",
    "经过layernorm层后，再经过Mask multi head attention层之后，会在+号处再次和原始的输入进行相加，这样的做法能够提高训练的稳定性。有兴趣的同学可以从梯度角度思考原因，或者搜索残差连接相关资料进行学习。\n",
    "之后再同样经过一层layernorm和feedforwad之后，就可以得到block块的输出了。\n",
    "即 x' = x+MHA(LN(x)), y = FFN(LN(x'))+x'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, embed_size: int, n_heads: int, seq_len: int, num_experts: int, active_experts: int):\n",
    "        super().__init__()\n",
    "        # 多头注意层\n",
    "        self.attention = MultiHeadAttention(n_heads, embed_size // n_heads, seq_len, embed_size)\n",
    "        # SparseMoE\n",
    "        self.sparse_moe = SparseMoE(embed_size, num_experts, active_experts)\n",
    "        # 前馈层\n",
    "        self.feed_forward = nn.Sequential(nn.Linear(embed_size, 4 * embed_size), nn.ReLU(), nn.Linear(4 * embed_size, embed_size))\n",
    "        # 常规化层\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "        self.ln3 = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 多头注意力\n",
    "        attn_output = self.attention(inputs)\n",
    "        attn_output = self.ln1(inputs + attn_output) \n",
    "        # SparseMoE\n",
    "        moe_output = self.sparse_moe(attn_output)\n",
    "        moe_output = self.ln2(attn_output + moe_output)\n",
    "        # 前馈层\n",
    "        ff_output = self.feed_forward(moe_output)\n",
    "        output = self.ln3(moe_output + ff_output) \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoETransformer(nn.Module):\n",
    "    def __init__(self, vocab_size:int, seq_len:int, embed_size:int, n_layers:int, n_heads:int, num_experts:int, active_experts:int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.embed_size = embed_size\n",
    "        # 词汇嵌入、位置编码\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(seq_len, embed_size)\n",
    "        # Transformer块\n",
    "        self.blocks = nn.ModuleList([Block(embed_size, n_heads, seq_len, num_experts, active_experts) for _ in range(n_layers)])\n",
    "        # 层常规化并且输出\n",
    "        self.layer_norm = nn.LayerNorm(embed_size)\n",
    "        self.output_layer = nn.Linear(embed_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        batch_size, seq_len = inputs.shape\n",
    "        # 为每个位置创建id\n",
    "        position_ids = torch.arange(seq_len, device=inputs.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        # 嵌入\n",
    "        token_embeddings = self.token_embedding(inputs)\n",
    "        position_embeddings = self.position_embedding(position_ids)  \n",
    "        embeddings = token_embeddings + position_embeddings  \n",
    "        # 从Transfromer块传过去\n",
    "        x = embeddings\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.output_layer(x)\n",
    "        # 如果有标签，计算损失\n",
    "        if labels is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return logits, loss\n",
    "    \n",
    "\n",
    "\n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        encoded_input = tokenizer.encode(inputs)\n",
    "        encoded_input = torch.tensor(encoded_input).unsqueeze(0) \n",
    "        device = next(self.parameters()).device  \n",
    "        encoded_input = encoded_input.to(device)\n",
    "\n",
    "        if encoded_input.size(1) > self.seq_len:\n",
    "            encoded_input = encoded_input[:, :self.seq_len]\n",
    "\n",
    "        generated = encoded_input\n",
    "        for _ in range(max_new_tokens):\n",
    "            if generated.size(1) > self.seq_len:\n",
    "                generated_input = generated[:, -self.seq_len:]\n",
    "            else:\n",
    "                generated_input = generated\n",
    "\n",
    "            logits, _ = self.forward(generated_input)\n",
    "            last_logits = logits[:, -1, :]\n",
    "            next_token_ids = torch.argmax(last_logits, dim=-1)\n",
    "            next_token_ids = next_token_ids.unsqueeze(-1)\n",
    "            generated = torch.cat([generated, next_token_ids], dim=1)\n",
    "\n",
    "        generated_ids = generated[0].tolist()\n",
    "        generated_text = tokenizer.decode(generated_ids)\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练循环\n",
    "\n",
    "如果你已经完成了模型定义等内容，训练的过程实际上在高度封装的 Pytorch 库中非常简单, 因为你并不需要写对应的反向传播。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss \n",
    "\n",
    "Loss 用来**衡量**模型预测与真实值之间的**差距**。\n",
    "\n",
    "常见的几个 Loss 函数：\n",
    "\n",
    "* 交叉熵：$\\text{CrossEntropy Loss} = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_i)$\n",
    "* 均方误差：$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$\n",
    "* 绝对误差：$\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y_i}|$\n",
    "\n",
    "不同的 loss 对应不同的优化目标，如果写错 loss 函数会导致模型不收敛/性能很差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练循环\n",
    "\n",
    "当我们写好 Optimizer 和 Loss 之后，对应的训练循环就十分简单了。\n",
    "\n",
    "我们只需要做以下事情：\n",
    "\n",
    "* 从 dataloader 里面拿到一个 batch 的数据以及标签\n",
    "* 将数据送入模型，进行前向传播\n",
    "* 拿到模型输出的 logits\n",
    "* 将 logits 和 标签进行 loss 计算\n",
    "* 用 Optimizer \n",
    "    * 清空梯度\n",
    "    * 反向传播\n",
    "    * 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epoch, device):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(inputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch} Loss: {total_loss / len(dataloader)}')\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, epoch, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            logits, loss = model(inputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    print(f'Epoch {epoch} Validation Loss: {total_loss / len(dataloader)}')\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据加载器\n",
    "train_dataloader, valid_dataloader = create_dataloader('input.txt', tokenizer, chunk_size=50, batch_size=512)\n",
    "\n",
    "# 初始化模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SparseMoETransformer(vocab_size=len(tokenizer.char2index), seq_len=50, embed_size=64, n_layers=3, n_heads=8, num_experts=8, active_experts=2\n",
    "                            ).to(device)\n",
    "\n",
    "# 训练模型\n",
    "def run(model, train_dataloader, valid_dataloader, device, epochs=10):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, train_dataloader, epoch, device)\n",
    "        valid_loss = validate(model, valid_dataloader, epoch, device)\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        print(f'Epoch {epoch} Train Loss: {train_loss}, Valid Loss: {valid_loss}')\n",
    "\n",
    "    # 绘图输出损失\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(epochs), train_losses, label='Train Loss')\n",
    "    plt.plot(range(epochs), valid_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 运行，训练轮次依次选择5,10,20，观察不同效果\n",
    "run(model, train_dataloader, valid_dataloader, device, epochs=20)\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "# 加载模型\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "# 测试生成文本，最多300词，选择五个句子进行测试\n",
    "generated_text = model.generate(\"I have a dog.\", tokenizer=tokenizer,max_new_tokens=300)\n",
    "print(generated_text)\n",
    "\n",
    "generated_text = model.generate(\"You are so fool!\", tokenizer=tokenizer,max_new_tokens=300)\n",
    "print(generated_text)\n",
    "\n",
    "generated_text = model.generate(\"Where is the God?\", tokenizer=tokenizer,max_new_tokens=300)\n",
    "print(generated_text)\n",
    "\n",
    "generated_text = model.generate(\"How are you?\", tokenizer=tokenizer,max_new_tokens=300)\n",
    "print(generated_text)\n",
    "\n",
    "generated_text = model.generate(\"Please tell me the truth\", tokenizer=tokenizer,max_new_tokens=300)\n",
    "print(generated_text)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
